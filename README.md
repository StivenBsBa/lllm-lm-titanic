# Proyecto End-to-End: ML + LLM (PredicciÃ³n Titanic)

Este repositorio contiene un proyecto completo de Machine Learning y LLM, desde el anÃ¡lisis de datos hasta el despliegue de una API con pruebas y documentaciÃ³n. El objetivo es predecir la supervivencia de los pasajeros del Titanic y responder preguntas sobre el proyecto utilizando un sistema RAG.

---

## ğŸš€ Features

- **Modelo de ML Supervisado:** Un pipeline de clasificaciÃ³n (RegresiÃ³n LogÃ­stica) para predecir la supervivencia.
- **Asistente LLM (RAG):** Un sistema de eneration que responde preguntas basÃ¡ndose en documentos del proyecto para evitar alucinaciones.
- **API Robusta:** Un servicio web construido con FastAPI que expone dos endpoints: `/predict` para el modelo de ML y `/llm/ask` para el asistente RAG.
- **Pruebas Automatizadas:** Pruebas de integraciÃ³n para la API usando `pytest`.
- **DocumentaciÃ³n Completa:** Incluye un `MODEL_CARD.md` para el modelo de ML y un `LLM_REPORT.md` para el componente de IA Generativa.
- **Reproducibilidad:** El proyecto estÃ¡ diseÃ±ado para ser completamente reproducible, con scripts para descarga de datos, entrenamiento y un entorno definido.

---

## ğŸ› ï¸ Tech Stack

- **Lenguaje:** Python 3.10+
- **AnÃ¡lisis y Modelado:** Pandas, Scikit-learn, NumPy
- **API:** FastAPI, Uvicorn
- **Componente LLM:** LangChain, Google Gemini API, FAISS, Sentence-Transformers
- **Pruebas:** Pytest

---

## ğŸ“‚ Estructura del Proyecto

```
â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ routers/
â”‚   â”‚   â”œâ”€â”€ llm.py
â”‚   â”‚   â””â”€â”€ ml.py
â”‚   â”œâ”€â”€ schemas/
â”‚   â”‚   â”œâ”€â”€ llm_query.py
â”‚   â”‚   â””â”€â”€ passenger.py
â”‚   â””â”€â”€ main.py
â”œâ”€â”€ artifacts/
â”‚   â””â”€â”€ logistic_regression_pipeline.joblib
â”œâ”€â”€ data/
â”‚   â””â”€â”€ get_data.py
â”œâ”€â”€ eval/
â”‚   â”œâ”€â”€ eval_ml.py
â”‚   â””â”€â”€ llm_eval.py
â”œâ”€â”€ knowledge_base/
â”‚   â””â”€â”€ project_summary.txt
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_eda.ipynb
â”‚   â””â”€â”€ 02_baseline.ipynb
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ llm/
â”‚   â”‚   â””â”€â”€ rag.py
â”‚   â”œâ”€â”€ ml/
â”‚   â”‚   â”œâ”€â”€ predict.py
â”‚   â”‚   â””â”€â”€ train.py
â”‚   â””â”€â”€ utils/
â”‚       â””â”€â”€ preprocessing.py
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_llm.py
â”‚   â””â”€â”€ test_ml.py
â”œâ”€â”€ vector_store/
â”‚   â”œâ”€â”€ index.faiss
â”‚   â””â”€â”€ index.pkl
â”œâ”€â”€ LLM_REPORT.md
â”œâ”€â”€ MODEL_CARD.md
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt
```

---

## ğŸ“‹ Â¿QuÃ© se hizo?

1. **Procesamiento de Datos:** Limpieza y transformaciÃ³n de los datos originales del Titanic para crear conjuntos de entrenamiento y prueba.
2. **Entrenamiento de Modelos ML:** Se entrenÃ³ un modelo de regresiÃ³n logÃ­stica y se guardÃ³ el pipeline en `artifacts/`.
3. **ImplementaciÃ³n de LLM y RAG:** Se desarrollÃ³ un sistema de recuperaciÃ³n aumentada por generaciÃ³n (RAG) usando LangChain y FAISS, almacenando los Ã­ndices en `vector_store/`.
4. **API REST:** Se creÃ³ una API con FastAPI para exponer endpoints de predicciÃ³n y consulta LLM.
5. **EvaluaciÃ³n:** Scripts y notebooks para analizar el desempeÃ±o de los modelos y el sistema LLM.
6. **Pruebas:** Pruebas automatizadas para asegurar la robustez del sistema.

## ğŸ—ï¸ Â¿CÃ³mo se trabaja?

### 0. ConfiguraciÃ³n de API Key de Google Gemini

Antes de usar el sistema LLM, necesitas configurar tu API key de Google Gemini:

1. ObtÃ©n tu API key desde [Google AI Studio](https://makersuite.google.com/app/apikey)
2. Crea un archivo `.env` en la raÃ­z del proyecto con:
   ```
   GOOGLE_API_KEY=tu_api_key_aqui
   ```

### 1. InstalaciÃ³n de dependencias

```bash
pip install -r requirements.txt
```

### 2. Procesamiento de datos

```bash
python data/get_data.py
```

### 3. Entrenamiento ML

```bash
python3 -m src.ml.train
```

- Pipeline guardado en `/artifacts/logistic_regression_pipeline.joblib`

### 4. Inferencia ML

```bash
python3 -m src.ml.predict
```

### 5. EvaluaciÃ³n ML

```bash
python eval/eval_ml.py
```

### 6. Crear Vector Store RAG

```bash
python src/llm/rag.py
```

### 7. EvaluaciÃ³n LLM / RAG

```bash
python eval/llm_eval.py
```

### 8. Correr API FastAPI

```bash
uvicorn api.main:app --reload
```

Endpoints disponibles:

- `POST /predict` â†’ Recibe features del pasajero y devuelve predicciÃ³n + probabilidad.
- `POST /llm/ask` â†’ Recibe pregunta y devuelve respuesta + metadatos (citaciones, latencia, tokens).

### 9. Pruebas

```bash
pytest tests/
```

### 10. Notebooks

Revisar `notebooks/` para EDA y experimentos de modelado.

## ğŸ’¡ Recomendaciones

- Revisa los notebooks para entender el flujo de datos y experimentos realizados.
- Consulta la documentaciÃ³n en los archivos `MODEL_CARD.md` y `LLM_REPORT.md` para detalles tÃ©cnicos de los modelos.
